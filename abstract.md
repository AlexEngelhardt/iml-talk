# Abstract

Complex machine learning models make better predictions, but at the cost of turning into an unexplainable black-box model. In this talk, we'll look into some techniques that allow us to explain why a model makes a specific prediction.

# Description

We all love linear regression for its interpretability: Increase square
meters by 1, that leads to rent going up by 8 euros. A human can easily
understand why this model made a certain prediction.

Complex machine learning models like tree aggregates or neural networks
usually make better predictions, but this comes at a price: it's hard to
understand these models.

In this talk, we'll look at a few methods to pry open these models and gain
some insights.
   
Specifically, the topics covered are

- What makes a model interpretable?
  - Linear models, trees, decision rules
- Model-agnostic methods for interpretability
  - Permutation Feature Importance
  - Partial dependence plots (PDPs)
  - Individual conditional expectations (ICEs)
- Example-based explanations
- The future of machine learning

